# Locust Compare

Compare performance results between two Locust runs and show changes relative to a base run. Works with both Locust CSV `report.csv` outputs and the per-feature HTML reports generated by the Locust web UI.

## Features

- Compare any two runs (base vs. current).
- Parses CSV `report.csv` for aggregated and per-endpoint metrics.
- Parses per-feature `.html` pages and compares the latest history sample.
- Outputs human-readable tables or machine-friendly JSON.

## Requirements

- Python 3.8+ (no third-party dependencies).

## Installation

### With uvx (recommended)

Run directly from GitHub without cloning:

```bash
uvx --from git+https://github.com/dev-ankit/locust-compare.git locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

Or from a local directory:

```bash
uvx --from . locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

Or from a cloned repository:

```bash
git clone https://github.com/dev-ankit/locust-compare.git
cd locust-compare
uvx --from . locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

Once published to PyPI, you can run without any prefix:

```bash
uvx locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

### With pip

```bash
pip install .
locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

### Direct execution

```bash
python3 compare_runs.py test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

## Quick Start

- Compare two run directories (each containing a `report.csv` and HTML files):

```bash
locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294
```

- Compare two specific CSV files:

```bash
locust-compare test_runs/HTML-Report-292/report.csv test_runs/HTML-Report-294/report.csv
```

- JSON output for scripting:

```bash
locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294 --json
```

- Colorize output and show verdicts (green=better, red=worse):

```bash
locust-compare test_runs/HTML-Report-292 test_runs/HTML-Report-294 --color
```

Exit code is `0` on success and `1` on error.

## What It Compares

From CSV `report.csv` (Aggregated and each request row):
- Requests/s, Request Count, Failure Count
- Average, Median, Min, Max Response Time
- Percentiles: 50%, 66%, 75%, 80%, 90%, 95%, 98%, 99%, 99.9%, 99.99%, 100% (if present)

From HTML feature pages (last entry in `window.templateArgs.history`):
- Requests/s (`current_rps`)
- Average Response Time (`total_avg_response_time`)
- 50% (`response_time_percentile_0.5`)
- 95% (`response_time_percentile_0.95`)

If a metric is not available for an item, it is shown as `-`.

## Example Output (truncated)

<img width="598" height="255" alt="image" src="https://github.com/user-attachments/assets/f5394045-6d1e-498e-aa3f-624928ec70a7" />


## JSON Schema

The `--json` output is a single JSON object containing keys for each compared item.

- CSV items use their request name; the aggregated row is keyed as `Aggregated`.
- HTML feature pages are keyed as `HTML:<feature_file_stem>`.

Each item maps metric names to an object with:

```
{
  "base": number | null,
  "current": number | null,
  "diff": number | null,
  "pct_change": number | null
}
```

Example (truncated):

```
{
  "Aggregated": {
    "Requests/s": {"base": 268.623, "current": 196.786, "diff": -71.836, "pct_change": -26.72},
    "Average Response Time": {"base": 71.801, "current": 98.069, ...}
  },
  "HTML:conferences_widget_all_lists": {
    "Requests/s": {"base": 271.5, "current": 189.8, ...},
    "95%": {"base": 160, "current": 190, ...}
  }
}
```

## Notes & Limitations

- For HTML pages, only the last sample in `window.templateArgs.history` is compared. This typically represents the end-state of the run. If you prefer a different aggregation (mean/max), open an issue or adjust the code where noted.
- Request Count and Failure Count are not available from HTML pages and are displayed as `-`.
- If the base value is `0` or missing, percent change is shown as `-`.
- The tool skips non-feature HTML pages such as `htmlpublisher-wrapper.html`.
- The tool prints a Verdict column. By default, it evaluates improvements as:
  - Higher is better: `Requests/s`, `Request Count`.
  - Lower is better: all response-time metrics and percentiles, `Failure Count`, `Failures/s`.
  - Neutral (no verdict): other metrics (e.g., `Average Content Size`).

## Repository Layout

```
.
├── compare_runs.py     # CLI tool
└── test_runs/          # Sample Locust outputs for trying the tool
    ├── HTML-Report-292/
    └── HTML-Report-294/
```

## Contributing

Small and simple by design. If you need additional metrics, output formats, or aggregation modes, feel free to extend `compare_runs.py` or open a PR.
